# Project Documentation

## 1. Problem Statement
The goal is to design and implement a modular agentic automation system that takes a single product JSON file
and generates three machine readable content pages. The system must demonstrate clean system design, small
independent agents, template based generation, and a central orchestrator. The focus is on engineering structure
rather than domain enrichment.

## 2. Solution Overview
The system is built as a linear multi-agent pipeline. Each agent performs one responsibility and returns a validated Pydantic model.
The system takes a product object and generates three structured outputs:

1.  **FAQ Page**: An agent generates questions, and another answers them based on product data.
2.  **Product Page**: A structured JSON summary of the product's key features.
3.  **Comparison Page**: A comparison of the product against a randomly named, fictional competitor.

The system is fully deterministic and grounded in the input product fields. No external web lookup or research is
performed. The LLM is used only for language generation inside the provided templates.

## 3. Scopes and Assumptions
* The input is a single clean JSON object describing a product.
* No external data enrichment is done.
* The competitor product is fictional and auto generated.
* All outputs must be valid JSON.
* Agents do not share state. Everything is passed explicitly.
* Templates define the shape of the final JSON outputs.

## 4. System Architecture

### 4.1 Layered Architecture
The project follows a three layer architecture

1. Data Layer  
   Holds the product.json file and validates it using a Pydantic product model.

2. Agent Layer  
   Contains four independent agents  
   * Question Agent  
   * FAQ Agent  
   * Product Page Agent  
   * Comparison Agent  

3. Orchestrator Layer  
   Loads product data, calls each agent in sequence, bundles and returns outputs.

### 4.2 Workflow
Product JSON  
→ Question Agent  
→ FAQ Agent  
→ Product Page Agent  
→ Comparison Agent  
→ Orchestrator → Final Output

### 4.3 Design Principles
* Each agent has one clear responsibility.
* No hidden global state.
* All LLM calls use structured templates.
* Agents validate and extract JSON using regex before parsing.
* Agent outputs are validated against strict Pydantic models.

## 5. Agents (Final Behaviour)

### 5.1 Question Agent
*   Reads product fields to generate 10 meaningful, product-specific questions.
*   Returns a validated `QuestionList` Pydantic model.

### 5.2 FAQ Agent
*   Takes the list of generated questions and the product data.
*   Uses a highly explicit prompt to produce factual answers grounded only in the product fields.
*   Returns a validated `FAQOutput` Pydantic model.

### 5.3 Product Page Agent
*   Generates a clean, structured JSON summary of the product.
*   Returns a validated `ProductPage` Pydantic model.

### 5.4 Comparison Agent
*   Generates a fictional competitor with a name randomly selected from a predefined list (e.g., "Face Serum", "Radiant Glow Serum").
*   Compares the real and fictional products on similarities, differences, and recommendations.
*   Returns a validated `ComparisonPage` Pydantic model.

## 6. Data Models (Pydantic)
The system uses Pydantic models for strict data validation at each step.

*   `Product`: The canonical model for the input product data.
*   `QuestionList`: A model for the list of questions generated by the `QuestionAgent`.
*   `FAQItem`: A model for a single question-answer pair.
*   `FAQList`: A model containing a list of `FAQItem` objects.
*   `FAQOutput`: The final, nested model for the FAQ page.
*   `ProductPage`: The model for the structured product page.
*   `ComparisonPage`: The model for the product comparison page.

## 7. Template Engine
Templates are stored inside the templates folder.  
Each template is a text file that describes the target JSON shape and placeholders.  
Agents load templates and pass structured product fields into the template.

Templates used  

* question_template.txt  
* faq_template.txt  
* product_page_template.txt  
* comparison_template.txt  

## 8. Orchestration (`run_orchestration`)
A standalone `run_orchestration` function handles the entire pipeline:

1.  Initializes the `Orchestrator` class, which in turn initializes all agents.
2.  Calls the `Orchestrator.run()` method, which executes the agents in sequence:
    *   `QuestionGeneratorAgent` -> `QuestionList` model
    *   `FAQAgent` -> `FAQOutput` model
    *   `ProductPageAgent` -> `ProductPage` model
    *   `ComparisonAgent` -> `ComparisonPage` model
3.  The `run()` method returns a dictionary of Pydantic models.
4.  The `run_orchestration` function then calls a `Writer` class to save the `.model_dump()` output of each model to a separate JSON file in the `outputs/` directory.

## 9. Execution
You run the project using

python main.py

This loads the product, runs all agents and prints the final JSON outputs.

## 10. Security
*   API keys are loaded from a `.env` file using `python-dotenv`.
*   The system is configured to use the **Groq API** key (`GROQ_API_KEY`).
*   Keys are never committed to the repository.

## 11. Evaluation Criteria
* All three output pages are valid JSON.
* FAQ contains answers for each generated question.
* Comparison includes a non Pro Max fictional competitor.
* Agents are modular and stateless.
* Orchestrator is explicit and linear.
* Templates define the shape of JSON outputs.
* No ambiguous or hidden logic.

# End of Document
